{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf48f6b6-7f33-495e-8416-e71f8ebb8562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in ./.local/lib/python3.8/site-packages (2.4.1+cu118)\n",
      "Requirement already satisfied: torchvision in ./.local/lib/python3.8/site-packages (0.19.1+cu118)\n",
      "Requirement already satisfied: numpy in ./.local/lib/python3.8/site-packages (1.24.4)\n",
      "Requirement already satisfied: matplotlib in ./.local/lib/python3.8/site-packages (3.7.3)\n",
      "Requirement already satisfied: tqdm in ./.local/lib/python3.8/site-packages (4.66.1)\n",
      "Requirement already satisfied: opencv-python in ./.local/lib/python3.8/site-packages (4.8.1.78)\n",
      "Requirement already satisfied: filelock in ./.local/lib/python3.8/site-packages (from torch) (3.12.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./.local/lib/python3.8/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in ./.local/lib/python3.8/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in ./.local/lib/python3.8/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in ./.local/lib/python3.8/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in ./.local/lib/python3.8/site-packages (from torch) (2023.9.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in ./.local/lib/python3.8/site-packages (from torch) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in ./.local/lib/python3.8/site-packages (from torch) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in ./.local/lib/python3.8/site-packages (from torch) (11.8.87)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in ./.local/lib/python3.8/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in ./.local/lib/python3.8/site-packages (from torch) (11.11.3.6)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in ./.local/lib/python3.8/site-packages (from torch) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in ./.local/lib/python3.8/site-packages (from torch) (10.3.0.86)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in ./.local/lib/python3.8/site-packages (from torch) (11.4.1.48)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in ./.local/lib/python3.8/site-packages (from torch) (11.7.5.86)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.20.5 in ./.local/lib/python3.8/site-packages (from torch) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in ./.local/lib/python3.8/site-packages (from torch) (11.8.86)\n",
      "Requirement already satisfied: triton==3.0.0 in ./.local/lib/python3.8/site-packages (from torch) (3.0.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./.local/lib/python3.8/site-packages (from torchvision) (10.1.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.local/lib/python3.8/site-packages (from matplotlib) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.local/lib/python3.8/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.local/lib/python3.8/site-packages (from matplotlib) (4.43.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in ./.local/lib/python3.8/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.local/lib/python3.8/site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./.local/lib/python3.8/site-packages (from matplotlib) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.local/lib/python3.8/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in ./.local/lib/python3.8/site-packages (from matplotlib) (6.0.1)\n",
      "Requirement already satisfied: zipp>=3.1.0 in ./.local/lib/python3.8/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.16.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib) (1.14.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.local/lib/python3.8/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./.local/lib/python3.8/site-packages (from sympy->torch) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision numpy matplotlib tqdm opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b54fa330-559e-4ef7-aeea-64643b1b5f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a073deb-9ea5-4e50-b6de-f5f83bd112a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Available: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"GPU Available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11b96555-e0e8-4e0b-bda6-bf1e5179a01e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset downloaded and extracted successfully!\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import tarfile\n",
    "\n",
    "# Define dataset URL and paths\n",
    "dataset_url = \"https://efrosgans.eecs.berkeley.edu/pix2pix/datasets/edges2handbags.tar.gz\"\n",
    "dataset_path = \"datasets/\"\n",
    "\n",
    "# Create directory if not exists\n",
    "os.makedirs(dataset_path, exist_ok=True)\n",
    "\n",
    "# Download dataset\n",
    "dataset_file = dataset_path + \"edges2handbags.tar.gz\"\n",
    "urllib.request.urlretrieve(dataset_url, dataset_file)\n",
    "\n",
    "# Extract dataset\n",
    "with tarfile.open(dataset_file, \"r:gz\") as tar:\n",
    "    tar.extractall(dataset_path)\n",
    "\n",
    "# Remove tar file\n",
    "os.remove(dataset_file)\n",
    "\n",
    "print(\"Dataset downloaded and extracted successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f274c81e-a73c-4163-9d85-a0624189fae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d598574e-0947-4f5a-a561-23ffaf3d6209",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_paths = {\n",
    "    \"Handbags\": r\"/home/dell/AMLdata/edges2handbags\",\n",
    "    \"Shoes\": r\"/home/dell/AMLdata/edges2shoes\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11376cec-a323-4d45-bf63-77dd6ebbc39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "for name, path in dataset_paths.items():\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Error: {path} does not exist!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fc0c3394-f10c-4e7e-93b3-73eeb6eacde3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/dell/AMLdata/places', '/home/dell/AMLdata/edges2shoes', '/home/dell/AMLdata/edges2handbags']\n"
     ]
    }
   ],
   "source": [
    "base_path = \"/home/dell/AMLdata\"\n",
    "folders = [os.path.join(base_path, f) for f in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, f))]\n",
    "print(folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8d0e1901-8b4d-4a08-832e-2c7014be12ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dell/edges2shoes\n"
     ]
    }
   ],
   "source": [
    "folder_name = \"edges2shoes\"\n",
    "folder_path = os.path.abspath(folder_name)\n",
    "print(folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "caceab3c-5f3b-4d67-8fca-9ff783d14aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  # Resize for training\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])  # Normalize between -1 and 1\n",
    "])\n",
    "\n",
    "class PairedImageDataset(Dataset):\n",
    "    def __init__(self, root, transform=None):\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.image_files = [os.path.join(root, f) for f in os.listdir(root) if f.endswith('.jpg')]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_files[idx]\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        # Split into (edge, real photo)\n",
    "        w, h = img.size\n",
    "        edge = img.crop((0, 0, w//2, h))  # Left half = Edge\n",
    "        real = img.crop((w//2, 0, w, h))  # Right half = Realistic photo\n",
    "\n",
    "        if self.transform:\n",
    "            edge = self.transform(edge)\n",
    "            real = self.transform(real)\n",
    "\n",
    "        return edge, real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45db5627-43a5-4505-a1f1-7ff10dc54c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking path: /home/dell/AMLdata/edges2handbags\n",
      "Checking path: /home/dell/AMLdata/edges2shoes\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "for name, path in dataset_paths.items():\n",
    "    print(f\"Checking path: {path}\")\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Error: {path} does not exist!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9225721-db05-47ee-886c-63cb33f5b19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairedImageDataset(Dataset):\n",
    "    def __init__(self, root, transform=None):\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "\n",
    "        print(f\"Checking path: {root}\")  # Debugging\n",
    "\n",
    "        if not os.path.exists(root):\n",
    "            raise FileNotFoundError(f\"Dataset directory not found: {root}\")\n",
    "\n",
    "        self.image_files = [os.path.join(root, f) for f in os.listdir(root) if f.endswith('.jpg')]\n",
    "\n",
    "        if len(self.image_files) == 0:\n",
    "            print(f\"Warning: No images found in {root}!\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34784288-5efe-4db0-96a4-899428a5439e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in dataset folder: ['val', 'train']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "dataset_path = \"/home/dell/AMLdata/edges2shoes\"  # Change to the dataset folder you want to check\n",
    "print(\"Files in dataset folder:\", os.listdir(dataset_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "777bca12-a937-4b0f-b5b1-4e83597b75a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairedImageDataset(Dataset):\n",
    "    def __init__(self, root, transform=None):\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "\n",
    "        # Recursively collect all image file paths inside train/ and val/\n",
    "        self.image_files = []\n",
    "        for subdir in [\"train\", \"val\"]:  # Look inside both train/ and val/\n",
    "            folder_path = os.path.join(root, subdir)\n",
    "            if os.path.exists(folder_path):  # Ensure the folder exists\n",
    "                self.image_files.extend(\n",
    "                    [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.lower().endswith(('.jpg', '.png', '.jpeg'))]\n",
    "                )\n",
    "\n",
    "        # Debug: Check if images are found\n",
    "        print(f\"Found {len(self.image_files)} images in {root}\")\n",
    "\n",
    "        if len(self.image_files) == 0:\n",
    "            raise FileNotFoundError(f\"No images found in {root}/train or {root}/val\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1f1b457-f9c6-4a64-9396-9fc459bdc7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairedImageDataset(Dataset):\n",
    "    def __init__(self, root, transform=None):\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "\n",
    "        # Recursively collect all image file paths from any subfolders\n",
    "        self.image_files = []\n",
    "        for dirpath, _, filenames in os.walk(root):  # Walk through all subfolders\n",
    "            self.image_files.extend(\n",
    "                [os.path.join(dirpath, f) for f in filenames if f.lower().endswith(('.jpg', '.png', '.jpeg'))]\n",
    "            )\n",
    "\n",
    "        # Debug: Print how many images were found\n",
    "        print(f\"Found {len(self.image_files)} images in {root}\")\n",
    "\n",
    "        if len(self.image_files) == 0:\n",
    "            raise FileNotFoundError(f\"No images found in {root} or its subdirectories!\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "660c8b76-341e-48fc-b50c-13ac51acb6d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 138767 images in /home/dell/AMLdata/edges2handbags\n",
      "Found 50025 images in /home/dell/AMLdata/edges2shoes\n",
      "Handbags Dataset Loaded: 138767 paired images\n",
      "Shoes Dataset Loaded: 50025 paired images\n"
     ]
    }
   ],
   "source": [
    "datasets_dict = {name: PairedImageDataset(path, transform=transform) for name, path in dataset_paths.items()}\n",
    "batch_size = 16\n",
    "dataloaders = {name: DataLoader(ds, batch_size=batch_size, shuffle=True) for name, ds in datasets_dict.items()}\n",
    "\n",
    "for name, ds in datasets_dict.items():\n",
    "    print(f\"{name} Dataset Loaded: {len(ds)} paired images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4bc33078-aa29-42ba-b230-b0ca0d35e016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator Model Created!\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class UNetGenerator(nn.Module):\n",
    "    def __init__(self, input_channels=3, output_channels=3, features=64):\n",
    "        super(UNetGenerator, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, features, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(features, features * 2, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(features * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(features * 2, features * 4, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(features * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(features * 4, features * 8, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(features * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(features * 8, features * 4, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(features * 4),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.ConvTranspose2d(features * 4, features * 2, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(features * 2),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.ConvTranspose2d(features * 2, features, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(features),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.ConvTranspose2d(features, output_channels, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# Initialize Generator\n",
    "generator = UNetGenerator().cuda()\n",
    "\n",
    "print(\"Generator Model Created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a595a33-0d74-46c3-9fdb-b16f69445b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator Model Created!\n"
     ]
    }
   ],
   "source": [
    "class PatchDiscriminator(nn.Module):\n",
    "    def __init__(self, input_channels=6, features=64):\n",
    "        super(PatchDiscriminator, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, features, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(features, features * 2, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(features * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(features * 2, features * 4, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(features * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(features * 4, 1, kernel_size=4, stride=1, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, edge, real):\n",
    "        x = torch.cat([edge, real], dim=1)  # Concatenate (edge, real/generated)\n",
    "        return self.model(x)\n",
    "\n",
    "# Initialize Discriminator\n",
    "discriminator = PatchDiscriminator().cuda()\n",
    "\n",
    "print(\"Discriminator Model Created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b068e607-31b0-47b2-aaef-62e10e9d0126",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairedImageDataset(Dataset):\n",
    "    def __init__(self, root, transform=None):\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.image_files = sorted([os.path.join(root, f) for f in os.listdir(root) if f.endswith('.jpg')])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_files[idx]\n",
    "\n",
    "        # Load image and ensure it's valid\n",
    "        try:\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_path}: {e}\")\n",
    "            return None  # Skip bad images\n",
    "\n",
    "        # Ensure the image is split correctly into (edge, real)\n",
    "        w, h = img.size\n",
    "        if w < 2:  # Ensure width is large enough to split\n",
    "            print(f\"Skipping image {img_path}, width too small!\")\n",
    "            return None\n",
    "\n",
    "        edge = img.crop((0, 0, w//2, h))  # Left half\n",
    "        real = img.crop((w//2, 0, w, h))  # Right half\n",
    "\n",
    "        if self.transform:\n",
    "            edge = self.transform(edge)\n",
    "            real = self.transform(real)\n",
    "\n",
    "        return edge, real  # Ensure this returns correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d1f7da30-00a3-4b08-9b7f-2a70fc5697f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import glob\n",
    "\n",
    "# Define image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  # Resize for training\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])  # Normalize between -1 and 1\n",
    "])\n",
    "\n",
    "class PairedImageDataset(Dataset):\n",
    "    def __init__(self, root, transform=None):\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "\n",
    "        # Recursively find all images inside subfolders\n",
    "        self.image_files = sorted(glob.glob(os.path.join(root, \"**/*.jpg\"), recursive=True))\n",
    "\n",
    "        if len(self.image_files) == 0:\n",
    "            raise RuntimeError(f\"No images found in {root}. Check folder structure!\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_files[idx]\n",
    "\n",
    "        try:\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_path}: {e}\")\n",
    "            return self.__getitem__((idx + 1) % len(self))  # Skip to next image\n",
    "\n",
    "        # Ensure images are split correctly (edges | real photo)\n",
    "        w, h = img.size\n",
    "        if w < 2:\n",
    "            print(f\"Skipping {img_path}, width too small!\")\n",
    "            return self.__getitem__((idx + 1) % len(self))\n",
    "\n",
    "        edge = img.crop((0, 0, w//2, h))  # Left half = Edge\n",
    "        real = img.crop((w//2, 0, w, h))  # Right half = Realistic photo\n",
    "\n",
    "        if self.transform:\n",
    "            edge = self.transform(edge)\n",
    "            real = self.transform(real)\n",
    "\n",
    "        return edge, real  # Ensure this returns valid tensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "69db5287-e266-41eb-98ba-7b847ed53302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handbags Dataset Loaded: 138767 paired images\n",
      "Shoes Dataset Loaded: 50025 paired images\n"
     ]
    }
   ],
   "source": [
    "dataset_paths = {\n",
    "    \"Handbags\": r\"/home/dell/AMLdata/edges2handbags\",\n",
    "    \"Shoes\": r\"/home/dell/AMLdata/edges2shoes\"\n",
    "}\n",
    "\n",
    "# Load datasets\n",
    "datasets_dict = {name: PairedImageDataset(path, transform=transform) for name, path in dataset_paths.items()}\n",
    "batch_size = 16\n",
    "dataloaders = {name: DataLoader(ds, batch_size=batch_size, shuffle=True) for name, ds in datasets_dict.items()}\n",
    "\n",
    "# Print dataset sizes\n",
    "for name, ds in datasets_dict.items():\n",
    "    print(f\"{name} Dataset Loaded: {len(ds)} paired images\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "752dbcaa-92a8-4c42-ab5e-adc92ab02087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edge batch shape: torch.Size([16, 3, 256, 256])\n",
      "Real batch shape: torch.Size([16, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "for edge, real in dataloaders[\"Shoes\"]:\n",
    "    print(\"Edge batch shape:\", edge.shape)\n",
    "    print(\"Real batch shape:\", real.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "05e14bb4-f7fa-4b5b-819e-66cb7079f78f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss functions and optimizers initialized!\n"
     ]
    }
   ],
   "source": [
    "criterion_gan = nn.BCELoss()\n",
    "l1_loss = nn.L1Loss()\n",
    "\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "print(\"Loss functions and optimizers initialized!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ea73f99b-1bf2-4044-b729-4fbc1cacde41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Handbags Dataset Loaded: 138767 paired images\n",
      " Shoes Dataset Loaded: 50025 paired images\n"
     ]
    }
   ],
   "source": [
    "for name, ds in datasets_dict.items():\n",
    "    print(f\" {name} Dataset Loaded: {len(ds)} paired images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "31771da6-14c5-4a2d-9d73-2ddc4543afb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Save Function BEFORE Training Function\n",
    "def save_trained_models(dataset_name, generator, discriminator):\n",
    "    save_dir = os.getcwd()  # Get current working directory\n",
    "   \n",
    "    # Save full models\n",
    "    torch.save(generator, os.path.join(save_dir, f\"generator_{dataset_name}.pth\"))\n",
    "    torch.save(discriminator, os.path.join(save_dir, f\"discriminator_{dataset_name}.pth\"))\n",
    "\n",
    "    # Save only model weights\n",
    "    torch.save(generator.state_dict(), os.path.join(save_dir, f\"generator_weights_{dataset_name}.pth\"))\n",
    "    torch.save(discriminator.state_dict(), os.path.join(save_dir, f\"discriminator_weights_{dataset_name}.pth\"))\n",
    "\n",
    "    print(f\" {dataset_name} models saved successfully at {save_dir}!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c3d44c4a-05c6-4c28-b4d3-901b890a7b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training cGAN on Shoes Dataset...\n",
      "\n",
      "Epoch 1/100 - D Loss: 0.1250 - G Loss: 20.9759\n",
      "Epoch 2/100 - D Loss: 0.2635 - G Loss: 20.1871\n",
      "Epoch 3/100 - D Loss: 0.2256 - G Loss: 16.1376\n",
      "Epoch 4/100 - D Loss: 0.7274 - G Loss: 12.9691\n",
      "Epoch 5/100 - D Loss: 0.2081 - G Loss: 19.6297\n",
      "Epoch 6/100 - D Loss: 0.0206 - G Loss: 26.2806\n",
      "Epoch 7/100 - D Loss: 0.0074 - G Loss: 21.7140\n",
      "Epoch 8/100 - D Loss: 0.0358 - G Loss: 22.1803\n",
      "Epoch 9/100 - D Loss: 0.0167 - G Loss: 24.1718\n",
      "Epoch 10/100 - D Loss: 0.0057 - G Loss: 29.2349\n",
      "Epoch 11/100 - D Loss: 0.2091 - G Loss: 16.4510\n",
      "Epoch 12/100 - D Loss: 0.1238 - G Loss: 16.1530\n",
      "Epoch 13/100 - D Loss: 0.0004 - G Loss: 26.3516\n",
      "Epoch 14/100 - D Loss: 0.0187 - G Loss: 27.8696\n",
      "Epoch 15/100 - D Loss: 0.0117 - G Loss: 31.7295\n",
      "Epoch 16/100 - D Loss: 0.1466 - G Loss: 17.2259\n",
      "Epoch 17/100 - D Loss: 0.2813 - G Loss: 22.7406\n",
      "Epoch 18/100 - D Loss: 0.0154 - G Loss: 17.2596\n",
      "Epoch 19/100 - D Loss: 0.5401 - G Loss: 17.7276\n",
      "Epoch 20/100 - D Loss: 0.0435 - G Loss: 22.7132\n",
      "Epoch 21/100 - D Loss: 0.0205 - G Loss: 17.3843\n",
      "Epoch 22/100 - D Loss: 0.0281 - G Loss: 18.6909\n",
      "Epoch 23/100 - D Loss: 0.0434 - G Loss: 17.1663\n",
      "Epoch 24/100 - D Loss: 0.1826 - G Loss: 15.0750\n",
      "Epoch 25/100 - D Loss: 0.0127 - G Loss: 26.9093\n",
      "Epoch 26/100 - D Loss: 0.0124 - G Loss: 21.7860\n",
      "Epoch 27/100 - D Loss: 0.1022 - G Loss: 18.1290\n",
      "Epoch 28/100 - D Loss: 0.1398 - G Loss: 17.6691\n",
      "Epoch 29/100 - D Loss: 0.0087 - G Loss: 18.7424\n",
      "Epoch 30/100 - D Loss: 0.1330 - G Loss: 15.2525\n",
      "Epoch 31/100 - D Loss: 0.0292 - G Loss: 19.2673\n",
      "Epoch 32/100 - D Loss: 0.9059 - G Loss: 18.1543\n",
      "Epoch 33/100 - D Loss: 0.0714 - G Loss: 18.9923\n",
      "Epoch 34/100 - D Loss: 0.0194 - G Loss: 22.8836\n",
      "Epoch 35/100 - D Loss: 0.0009 - G Loss: 24.6116\n",
      "Epoch 36/100 - D Loss: 0.0005 - G Loss: 21.5562\n",
      "Epoch 37/100 - D Loss: 0.0644 - G Loss: 15.1795\n",
      "Epoch 38/100 - D Loss: 0.0092 - G Loss: 21.4829\n",
      "Epoch 39/100 - D Loss: 0.0000 - G Loss: 32.1298\n",
      "Epoch 40/100 - D Loss: 0.0136 - G Loss: 19.8759\n",
      "Epoch 41/100 - D Loss: 0.1043 - G Loss: 16.9032\n",
      "Epoch 42/100 - D Loss: 0.0364 - G Loss: 15.9472\n",
      "Epoch 43/100 - D Loss: 0.0161 - G Loss: 17.0760\n",
      "Epoch 44/100 - D Loss: 0.0001 - G Loss: 25.6662\n",
      "Epoch 45/100 - D Loss: 0.0000 - G Loss: 29.5028\n",
      "Epoch 46/100 - D Loss: 0.3045 - G Loss: 17.4579\n",
      "Epoch 47/100 - D Loss: 0.5944 - G Loss: 18.9577\n",
      "Epoch 48/100 - D Loss: 0.0379 - G Loss: 19.2261\n",
      "Epoch 49/100 - D Loss: 0.0160 - G Loss: 25.0783\n",
      "Epoch 50/100 - D Loss: 0.0022 - G Loss: 23.4698\n",
      "Epoch 51/100 - D Loss: 0.0011 - G Loss: 19.9573\n",
      "Epoch 52/100 - D Loss: 0.0001 - G Loss: 25.4907\n",
      "Epoch 53/100 - D Loss: 0.0134 - G Loss: 21.8812\n",
      "Epoch 54/100 - D Loss: 0.3736 - G Loss: 19.8639\n",
      "Epoch 55/100 - D Loss: 0.0075 - G Loss: 19.0345\n",
      "Epoch 56/100 - D Loss: 0.0211 - G Loss: 22.3229\n",
      "Epoch 57/100 - D Loss: 0.0020 - G Loss: 18.3645\n",
      "Epoch 58/100 - D Loss: 0.0000 - G Loss: 30.2564\n",
      "Epoch 59/100 - D Loss: 0.0078 - G Loss: 19.6371\n",
      "Epoch 60/100 - D Loss: 0.0432 - G Loss: 20.8231\n",
      "Epoch 61/100 - D Loss: 0.0180 - G Loss: 19.5254\n",
      "Epoch 62/100 - D Loss: 0.5193 - G Loss: 18.9511\n",
      "Epoch 63/100 - D Loss: 0.9118 - G Loss: 16.1059\n",
      "Epoch 64/100 - D Loss: 0.0002 - G Loss: 23.4073\n",
      "Epoch 65/100 - D Loss: 0.0188 - G Loss: 19.4432\n",
      "Epoch 66/100 - D Loss: 0.0007 - G Loss: 21.5244\n",
      "Epoch 67/100 - D Loss: 0.0989 - G Loss: 15.2536\n",
      "Epoch 68/100 - D Loss: 0.0034 - G Loss: 20.2837\n",
      "Epoch 69/100 - D Loss: 0.0104 - G Loss: 20.3510\n",
      "Epoch 70/100 - D Loss: 0.0016 - G Loss: 20.0506\n",
      "Epoch 71/100 - D Loss: 0.0039 - G Loss: 15.6884\n",
      "Epoch 72/100 - D Loss: 0.3420 - G Loss: 15.9365\n",
      "Epoch 73/100 - D Loss: 0.0084 - G Loss: 17.2420\n",
      "Epoch 74/100 - D Loss: 0.0029 - G Loss: 27.1875\n",
      "Epoch 75/100 - D Loss: 0.0049 - G Loss: 17.3045\n",
      "Epoch 76/100 - D Loss: 0.1078 - G Loss: 15.2448\n",
      "Epoch 77/100 - D Loss: 0.0239 - G Loss: 24.0920\n",
      "Epoch 78/100 - D Loss: 0.0369 - G Loss: 20.6059\n",
      "Epoch 79/100 - D Loss: 0.0419 - G Loss: 17.1820\n",
      "Epoch 80/100 - D Loss: 0.0050 - G Loss: 17.6020\n",
      "Epoch 81/100 - D Loss: 0.0030 - G Loss: 29.9493\n",
      "Epoch 82/100 - D Loss: 0.0102 - G Loss: 19.0299\n",
      "Epoch 83/100 - D Loss: 0.2888 - G Loss: 15.9348\n",
      "Epoch 84/100 - D Loss: 0.0040 - G Loss: 22.8671\n",
      "Epoch 85/100 - D Loss: 1.1477 - G Loss: 13.8102\n",
      "Epoch 86/100 - D Loss: 0.0004 - G Loss: 23.9182\n",
      "Epoch 87/100 - D Loss: 0.0001 - G Loss: 24.8365\n",
      "Epoch 88/100 - D Loss: 0.0120 - G Loss: 25.1469\n",
      "Epoch 89/100 - D Loss: 0.0147 - G Loss: 16.4692\n",
      "Epoch 90/100 - D Loss: 0.1882 - G Loss: 18.2514\n",
      "Epoch 91/100 - D Loss: 0.0940 - G Loss: 17.4034\n",
      "Epoch 92/100 - D Loss: 0.0097 - G Loss: 25.8990\n",
      "Epoch 93/100 - D Loss: 0.0445 - G Loss: 15.5064\n",
      "Epoch 94/100 - D Loss: 0.0278 - G Loss: 23.3460\n",
      "Epoch 95/100 - D Loss: 0.0022 - G Loss: 24.0939\n",
      "Epoch 96/100 - D Loss: 0.3151 - G Loss: 20.5238\n",
      "Epoch 97/100 - D Loss: 0.0344 - G Loss: 19.7030\n",
      "Epoch 98/100 - D Loss: 0.0160 - G Loss: 18.7729\n",
      "Epoch 99/100 - D Loss: 0.0236 - G Loss: 17.8655\n",
      "Epoch 100/100 - D Loss: 0.0007 - G Loss: 20.9095\n",
      " Shoes models saved successfully at /home/dell!\n",
      "\n",
      " Training cGAN on Handbags Dataset...\n",
      "\n",
      "Epoch 1/100 - D Loss: 0.0018 - G Loss: 35.7159\n",
      "Epoch 2/100 - D Loss: 0.0016 - G Loss: 45.0309\n",
      "Epoch 3/100 - D Loss: 0.0001 - G Loss: 41.9859\n",
      "Epoch 4/100 - D Loss: 0.0004 - G Loss: 38.0058\n",
      "Epoch 5/100 - D Loss: 0.0001 - G Loss: 43.5585\n",
      "Epoch 6/100 - D Loss: 0.0000 - G Loss: 37.6847\n",
      "Epoch 7/100 - D Loss: 0.0000 - G Loss: 46.3184\n",
      "Epoch 8/100 - D Loss: 0.0001 - G Loss: 37.6358\n",
      "Epoch 9/100 - D Loss: 0.0003 - G Loss: 40.3152\n",
      "Epoch 10/100 - D Loss: 0.0001 - G Loss: 37.3658\n",
      "Epoch 11/100 - D Loss: 0.0000 - G Loss: 43.0413\n",
      "Epoch 12/100 - D Loss: 0.0002 - G Loss: 37.3456\n",
      "Epoch 13/100 - D Loss: 0.0000 - G Loss: 48.5242\n",
      "Epoch 14/100 - D Loss: 0.0000 - G Loss: 39.7140\n",
      "Epoch 15/100 - D Loss: 0.0255 - G Loss: 40.2649\n",
      "Epoch 16/100 - D Loss: 0.0000 - G Loss: 37.3240\n",
      "Epoch 17/100 - D Loss: 0.0022 - G Loss: 42.1382\n",
      "Epoch 18/100 - D Loss: 0.0002 - G Loss: 39.8049\n",
      "Epoch 19/100 - D Loss: 0.0016 - G Loss: 35.9971\n",
      "Epoch 20/100 - D Loss: 0.0000 - G Loss: 42.0897\n",
      "Epoch 21/100 - D Loss: 0.0090 - G Loss: 35.1829\n",
      "Epoch 22/100 - D Loss: 0.0000 - G Loss: 44.4135\n",
      "Epoch 23/100 - D Loss: 0.0000 - G Loss: 44.2689\n",
      "Epoch 24/100 - D Loss: 0.0000 - G Loss: 51.7839\n",
      "Epoch 25/100 - D Loss: 0.0019 - G Loss: 25.9964\n",
      "Epoch 26/100 - D Loss: 0.0008 - G Loss: 37.1576\n",
      "Epoch 27/100 - D Loss: 0.0014 - G Loss: 29.6943\n",
      "Epoch 28/100 - D Loss: 0.0000 - G Loss: 36.4051\n",
      "Epoch 29/100 - D Loss: 0.0004 - G Loss: 43.1792\n",
      "Epoch 30/100 - D Loss: 0.0006 - G Loss: 35.7719\n",
      "Epoch 31/100 - D Loss: 0.0048 - G Loss: 40.1147\n",
      "Epoch 32/100 - D Loss: 0.0090 - G Loss: 36.9269\n",
      "Epoch 33/100 - D Loss: 0.0000 - G Loss: 33.7476\n",
      "Epoch 34/100 - D Loss: 0.0001 - G Loss: 40.5652\n",
      "Epoch 35/100 - D Loss: 0.0037 - G Loss: 33.4168\n",
      "Epoch 36/100 - D Loss: 0.0000 - G Loss: 43.3827\n",
      "Epoch 37/100 - D Loss: 0.0004 - G Loss: 31.9732\n",
      "Epoch 38/100 - D Loss: 0.0004 - G Loss: 37.0670\n",
      "Epoch 39/100 - D Loss: 0.0841 - G Loss: 35.5640\n",
      "Epoch 40/100 - D Loss: 0.0000 - G Loss: 37.0327\n",
      "Epoch 41/100 - D Loss: 0.0001 - G Loss: 38.3136\n",
      "Epoch 42/100 - D Loss: 0.0001 - G Loss: 43.6423\n",
      "Epoch 43/100 - D Loss: 0.0001 - G Loss: 38.7048\n",
      "Epoch 44/100 - D Loss: 0.0008 - G Loss: 31.9087\n",
      "Epoch 45/100 - D Loss: 0.0000 - G Loss: 36.9289\n",
      "Epoch 46/100 - D Loss: 0.0000 - G Loss: 41.9179\n",
      "Epoch 47/100 - D Loss: 0.0004 - G Loss: 39.6439\n",
      "Epoch 48/100 - D Loss: 0.0578 - G Loss: 31.9528\n",
      "Epoch 49/100 - D Loss: 0.0000 - G Loss: 37.5538\n",
      "Epoch 50/100 - D Loss: 0.0000 - G Loss: 40.2399\n",
      "Epoch 51/100 - D Loss: 0.0001 - G Loss: 39.6468\n",
      "Epoch 52/100 - D Loss: 0.0003 - G Loss: 36.1847\n",
      "Epoch 53/100 - D Loss: 0.0000 - G Loss: 37.8094\n",
      "Epoch 54/100 - D Loss: 0.0000 - G Loss: 37.0686\n",
      "Epoch 55/100 - D Loss: 0.0001 - G Loss: 38.6461\n",
      "Epoch 56/100 - D Loss: 0.0000 - G Loss: 36.1854\n",
      "Epoch 57/100 - D Loss: 0.0000 - G Loss: 46.1695\n",
      "Epoch 58/100 - D Loss: 0.0006 - G Loss: 36.4095\n",
      "Epoch 59/100 - D Loss: 0.0033 - G Loss: 29.1738\n",
      "Epoch 60/100 - D Loss: 0.0000 - G Loss: 36.5753\n",
      "Epoch 61/100 - D Loss: 0.0088 - G Loss: 32.4628\n",
      "Epoch 62/100 - D Loss: 0.0020 - G Loss: 39.0470\n",
      "Epoch 63/100 - D Loss: 0.0000 - G Loss: 40.5399\n",
      "Epoch 64/100 - D Loss: 0.0000 - G Loss: 44.7645\n",
      "Epoch 65/100 - D Loss: 0.0006 - G Loss: 32.2559\n",
      "Epoch 66/100 - D Loss: 0.0000 - G Loss: 42.7263\n",
      "Epoch 67/100 - D Loss: 0.0006 - G Loss: 35.4214\n",
      "Epoch 68/100 - D Loss: 0.0001 - G Loss: 40.1692\n",
      "Epoch 69/100 - D Loss: 0.0000 - G Loss: 39.0462\n",
      "Epoch 70/100 - D Loss: 0.0000 - G Loss: 45.0887\n",
      "Epoch 71/100 - D Loss: 0.0000 - G Loss: 37.7963\n",
      "Epoch 72/100 - D Loss: 0.0052 - G Loss: 30.8242\n",
      "Epoch 73/100 - D Loss: 0.0003 - G Loss: 35.8813\n",
      "Epoch 74/100 - D Loss: 0.0002 - G Loss: 32.8409\n",
      "Epoch 75/100 - D Loss: 0.0112 - G Loss: 29.4320\n",
      "Epoch 76/100 - D Loss: 0.0000 - G Loss: 43.2473\n",
      "Epoch 77/100 - D Loss: 0.0001 - G Loss: 34.9783\n",
      "Epoch 78/100 - D Loss: 0.0001 - G Loss: 39.4798\n",
      "Epoch 79/100 - D Loss: 0.0000 - G Loss: 44.6152\n",
      "Epoch 80/100 - D Loss: 0.0028 - G Loss: 41.7223\n",
      "Epoch 81/100 - D Loss: 0.0003 - G Loss: 34.6488\n",
      "Epoch 82/100 - D Loss: 0.0007 - G Loss: 35.1652\n",
      "Epoch 83/100 - D Loss: 0.0016 - G Loss: 30.4890\n",
      "Epoch 84/100 - D Loss: 0.0034 - G Loss: 31.5708\n",
      "Epoch 85/100 - D Loss: 0.0000 - G Loss: 39.4440\n",
      "Epoch 86/100 - D Loss: 0.0004 - G Loss: 37.2715\n",
      "Epoch 87/100 - D Loss: 0.0149 - G Loss: 32.8458\n",
      "Epoch 88/100 - D Loss: 0.0005 - G Loss: 32.5419\n",
      "Epoch 89/100 - D Loss: 0.0000 - G Loss: 40.9869\n",
      "Epoch 90/100 - D Loss: 0.0036 - G Loss: 33.8499\n",
      "Epoch 91/100 - D Loss: 0.0000 - G Loss: 44.4443\n",
      "Epoch 92/100 - D Loss: 0.0001 - G Loss: 37.8603\n",
      "Epoch 93/100 - D Loss: 0.0000 - G Loss: 41.9063\n",
      "Epoch 94/100 - D Loss: 0.0000 - G Loss: 41.0116\n",
      "Epoch 95/100 - D Loss: 0.0002 - G Loss: 33.7697\n",
      "Epoch 96/100 - D Loss: 0.0000 - G Loss: 45.4140\n",
      "Epoch 97/100 - D Loss: 0.0000 - G Loss: 40.1076\n",
      "Epoch 98/100 - D Loss: 0.1512 - G Loss: 33.6321\n",
      "Epoch 99/100 - D Loss: 0.0154 - G Loss: 36.9304\n",
      "Epoch 100/100 - D Loss: 0.0026 - G Loss: 37.6770\n",
      " Handbags models saved successfully at /home/dell!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "def train_cgan(dataset_name, dataloader):\n",
    "    print(f\"\\n Training cGAN on {dataset_name} Dataset...\\n\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "\n",
    "        for i, (edge, real) in enumerate(dataloader):\n",
    "            edge, real = edge.cuda(), real.cuda()\n",
    "\n",
    "            # Train Discriminator\n",
    "            optimizer_D.zero_grad()\n",
    "            fake_images = generator(edge)\n",
    "\n",
    "            real_output = discriminator(edge, real)\n",
    "            fake_output = discriminator(edge, fake_images.detach())\n",
    "\n",
    "            real_labels = torch.ones_like(real_output).cuda()\n",
    "            fake_labels = torch.zeros_like(fake_output).cuda()\n",
    "\n",
    "            real_loss = criterion_gan(real_output, real_labels)\n",
    "            fake_loss = criterion_gan(fake_output, fake_labels)\n",
    "            d_loss = (real_loss + fake_loss) / 2\n",
    "\n",
    "            d_loss.backward()\n",
    "            optimizer_D.step()\n",
    "\n",
    "            # Train Generator\n",
    "            optimizer_G.zero_grad()\n",
    "            fake_output = discriminator(edge, fake_images)\n",
    "            g_loss = criterion_gan(fake_output, real_labels) + 100 * l1_loss(fake_images, real)\n",
    "\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - D Loss: {d_loss.item():.4f} - G Loss: {g_loss.item():.4f}\")\n",
    "        \n",
    "    save_trained_models(dataset_name, generator, discriminator )\n",
    "\n",
    "\n",
    "\n",
    "# Train on all datasets\n",
    "for dataset_name in [\"Shoes\", \"Handbags\"]:\n",
    "    train_cgan(dataset_name, dataloaders[dataset_name])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
