{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a2e4b20-f24b-4f30-a299-d45f82fe72de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3a3a279e-fd63-4d4f-a2fe-2cf2ae8a96ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Places Train Dataset Loaded: 2428 images\n",
      "Places Validation Dataset Loaded: 608 images\n"
     ]
    }
   ],
   "source": [
    "class PlacesDataset(Dataset):\n",
    "    def __init__(self, root, transform=None, train=True, split_ratio=0.8):\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.image_files = sorted(glob.glob(os.path.join(root, \"**/*.jpg\"), recursive=True))\n",
    "\n",
    "        if len(self.image_files) == 0:\n",
    "            raise RuntimeError(f\"No images found in {root}. Check folder structure!\")\n",
    "\n",
    "        # Split dataset into train & validation\n",
    "        train_files, val_files = train_test_split(self.image_files, train_size=split_ratio, random_state=42)\n",
    "        self.image_files = train_files if train else val_files  # Use train or val set\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_files[idx]\n",
    "\n",
    "        try:\n",
    "            real = Image.open(img_path).convert(\"RGB\")  # Load real image\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_path}: {e}\")\n",
    "            return self.__getitem__((idx + 1) % len(self))  # Skip to next image\n",
    "\n",
    "        if self.transform:\n",
    "            real = self.transform(real)\n",
    "\n",
    "        return real  # Only return real image (no edges)\n",
    "\n",
    "# Define dataset path for Places\n",
    "places_path = r\"D:\\scribbles\\places\"\n",
    "\n",
    "\n",
    "# Define image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),  # Resize to match DCGAN generator output\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])  # Normalize to [-1,1] for Tanh activation\n",
    "])\n",
    "\n",
    "# Load train and validation datasets\n",
    "places_train_dataset = PlacesDataset(places_path, transform=transform, train=True)\n",
    "places_val_dataset = PlacesDataset(places_path, transform=transform, train=False)\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 16\n",
    "places_train_dataloader = DataLoader(places_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "places_val_dataloader = DataLoader(places_val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Print dataset sizes\n",
    "print(f\"Places Train Dataset Loaded: {len(places_train_dataset)} images\")\n",
    "print(f\"Places Validation Dataset Loaded: {len(places_val_dataset)} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e4c5837-38a6-4a14-855f-6bf1167c8195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Updated Generator for 128x128 resolution\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DCGANGenerator(nn.Module):\n",
    "    def __init__(self, latent_dim=100, img_channels=3, features_g=64):\n",
    "        super(DCGANGenerator, self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.ConvTranspose2d(latent_dim, features_g * 16, 4, 1, 0, bias=False),  # 4x4\n",
    "            nn.BatchNorm2d(features_g * 16),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(features_g * 16, features_g * 8, 4, 2, 1, bias=False),  # 8x8\n",
    "            nn.BatchNorm2d(features_g * 8),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(features_g * 8, features_g * 4, 4, 2, 1, bias=False),  # 16x16\n",
    "            nn.BatchNorm2d(features_g * 4),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(features_g * 4, features_g * 2, 4, 2, 1, bias=False),  # 32x32\n",
    "            nn.BatchNorm2d(features_g * 2),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(features_g * 2, features_g, 4, 2, 1, bias=False),  # 64x64\n",
    "            nn.BatchNorm2d(features_g),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(features_g, img_channels, 4, 2, 1, bias=False),  # 128x128\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.net(z)\n",
    "\n",
    "print(\" Updated Generator for 128x128 resolution\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "789eac77-7b19-4329-9904-f19dfcf4deae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Updated Discriminator for 128x128 resolution\n"
     ]
    }
   ],
   "source": [
    "class DCGANDiscriminator(nn.Module):\n",
    "    def __init__(self, img_channels=3, features_d=64):\n",
    "        super(DCGANDiscriminator, self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(img_channels, features_d, 4, 2, 1, bias=False),  # 128x128 → 64x64\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(features_d, features_d * 2, 4, 2, 1, bias=False),  # 64x64 → 32x32\n",
    "            nn.BatchNorm2d(features_d * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(features_d * 2, features_d * 4, 4, 2, 1, bias=False),  # 32x32 → 16x16\n",
    "            nn.BatchNorm2d(features_d * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(features_d * 4, features_d * 8, 4, 2, 1, bias=False),  # 16x16 → 8x8\n",
    "            nn.BatchNorm2d(features_d * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(features_d * 8, features_d * 16, 4, 2, 1, bias=False),  # 8x8 → 4x4\n",
    "            nn.BatchNorm2d(features_d * 16),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(features_d * 16, 1, 4, 1, 0, bias=False),  # 4x4 → 1x1\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        return self.net(img)\n",
    "\n",
    "print(\" Updated Discriminator for 128x128 resolution\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47ddb6f3-0c03-4d2f-bee6-836e7bea1fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Places Train Dataset Loaded: 2428 images\n",
      "Places Validation Dataset Loaded: 608 images\n"
     ]
    }
   ],
   "source": [
    "# Define dataset path for Places\n",
    "places_path = r\"D:\\scribbles\\places\"\n",
    "\n",
    "# Define image transformations (Set resolution to 128x128)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),  # Ensure all images are resized to 128x128\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])  # Normalize to [-1,1] for Tanh activation\n",
    "])\n",
    "\n",
    "# Load train and validation datasets\n",
    "places_train_dataset = PlacesDataset(places_path, transform=transform, train=True)\n",
    "places_val_dataset = PlacesDataset(places_path, transform=transform, train=False)\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 16  # Reduce if memory runs out\n",
    "places_train_dataloader = DataLoader(places_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "places_val_dataloader = DataLoader(places_val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Places Train Dataset Loaded: {len(places_train_dataset)} images\")\n",
    "print(f\"Places Validation Dataset Loaded: {len(places_val_dataset)} images\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3d83dfbf-add0-4968-a2fb-384e8838ad65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loss functions and optimizers initialized for Places dataset!\n"
     ]
    }
   ],
   "source": [
    "# Adjust Generator and Discriminator\n",
    "places_generator = DCGANGenerator(latent_dim=100)\n",
    "places_discriminator = DCGANDiscriminator()\n",
    "\n",
    "# Loss and Optimizers\n",
    "criterion = nn.BCELoss()\n",
    "places_optimizer_G = torch.optim.Adam(places_generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "places_optimizer_D = torch.optim.Adam(places_discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "print(\" Loss functions and optimizers initialized for Places dataset!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "519d829d-79ed-4101-84ce-374ab2fae473",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for DCGANGenerator:\n\tsize mismatch for net.0.weight: copying a param with shape torch.Size([100, 512, 4, 4]) from checkpoint, the shape in current model is torch.Size([100, 1024, 4, 4]).\n\tsize mismatch for net.1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for net.1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for net.1.running_mean: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for net.1.running_var: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for net.3.weight: copying a param with shape torch.Size([512, 256, 4, 4]) from checkpoint, the shape in current model is torch.Size([1024, 512, 4, 4]).\n\tsize mismatch for net.4.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for net.4.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for net.4.running_mean: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for net.4.running_var: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for net.6.weight: copying a param with shape torch.Size([256, 128, 4, 4]) from checkpoint, the shape in current model is torch.Size([512, 256, 4, 4]).\n\tsize mismatch for net.7.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for net.7.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for net.7.running_mean: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for net.7.running_var: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for net.9.weight: copying a param with shape torch.Size([128, 64, 4, 4]) from checkpoint, the shape in current model is torch.Size([256, 128, 4, 4]).\n\tsize mismatch for net.10.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for net.10.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for net.10.running_mean: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for net.10.running_var: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for net.12.weight: copying a param with shape torch.Size([64, 3, 4, 4]) from checkpoint, the shape in current model is torch.Size([128, 64, 4, 4]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Load the saved weights\u001b[39;00m\n\u001b[0;32m      6\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mplaces_dcgan_generator_weights.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m, map_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m places_generator\u001b[38;5;241m.\u001b[39mload_state_dict(checkpoint, strict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      8\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mplaces_dcgan_discriminator_weights.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m, map_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m places_discriminator\u001b[38;5;241m.\u001b[39mload_state_dict(checkpoint, strict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2153\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[0;32m   2148\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   2149\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2150\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[0;32m   2152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2154\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[0;32m   2155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for DCGANGenerator:\n\tsize mismatch for net.0.weight: copying a param with shape torch.Size([100, 512, 4, 4]) from checkpoint, the shape in current model is torch.Size([100, 1024, 4, 4]).\n\tsize mismatch for net.1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for net.1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for net.1.running_mean: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for net.1.running_var: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for net.3.weight: copying a param with shape torch.Size([512, 256, 4, 4]) from checkpoint, the shape in current model is torch.Size([1024, 512, 4, 4]).\n\tsize mismatch for net.4.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for net.4.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for net.4.running_mean: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for net.4.running_var: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for net.6.weight: copying a param with shape torch.Size([256, 128, 4, 4]) from checkpoint, the shape in current model is torch.Size([512, 256, 4, 4]).\n\tsize mismatch for net.7.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for net.7.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for net.7.running_mean: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for net.7.running_var: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for net.9.weight: copying a param with shape torch.Size([128, 64, 4, 4]) from checkpoint, the shape in current model is torch.Size([256, 128, 4, 4]).\n\tsize mismatch for net.10.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for net.10.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for net.10.running_mean: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for net.10.running_var: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for net.12.weight: copying a param with shape torch.Size([64, 3, 4, 4]) from checkpoint, the shape in current model is torch.Size([128, 64, 4, 4])."
     ]
    }
   ],
   "source": [
    "# Define the models\n",
    "places_generator = DCGANGenerator()\n",
    "places_discriminator = DCGANDiscriminator()\n",
    "\n",
    "# Load the saved weights\n",
    "checkpoint = torch.load(\"F:\\places_dcgan_generator_weights.pth\", map_location=\"cpu\")\n",
    "places_generator.load_state_dict(checkpoint, strict=False)\n",
    "checkpoint = torch.load(\"F:\\places_dcgan_discriminator_weights.pth\", map_location=\"cpu\")\n",
    "places_discriminator.load_state_dict(checkpoint, strict=False)\n",
    "\n",
    "# Set models to evaluation mode\n",
    "places_generator.eval()\n",
    "print(\" Generator loaded \")\n",
    "places_discriminator.eval()\n",
    "print(\" Discriminator loaded \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e4d987-1655-4f67-a0cd-c89aa60c2e42",
   "metadata": {},
   "source": [
    "# Project Real Images onto the DCGAN Latent Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "93f914d0-a087-4d5a-9204-f8b58715b729",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "from torchvision.models import VGG16_Weights\n",
    "\n",
    "vgg16 = models.vgg16(weights=VGG16_Weights.IMAGENET1K_V1).features[:16].eval()\n",
    "\n",
    "\n",
    "def perceptual_loss(gen_image, real_image):\n",
    "    \"\"\"\n",
    "    Computes perceptual loss using VGG16 features.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        real_features = vgg16(real_image)\n",
    "    gen_features = vgg16(gen_image)\n",
    "    return F.mse_loss(gen_features, real_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d361874-d503-4d90-8a01-b9c99a3a77ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0/500 - Loss: 0.895149\n",
      "Step 100/500 - Loss: 0.000091\n",
      "Step 200/500 - Loss: 0.000068\n",
      "Step 300/500 - Loss: 0.000058\n",
      "Step 400/500 - Loss: 0.000051\n",
      "Image projected onto latent space successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "\n",
    "def project_image_to_latent(image, generator, num_steps=500, lr=0.1):\n",
    "    \"\"\"\n",
    "    Projects a real image onto the DCGAN latent space by optimizing z.\n",
    "    \"\"\"\n",
    "    generator.eval()  # Set generator to evaluation mode\n",
    "\n",
    "    # Resize real image to match GAN output (128x128)\n",
    "    image = transforms.Resize((128, 128))(image)  # Ensure same size\n",
    "    image = image.unsqueeze(0)  # Add batch dimension\n",
    "    image = (image / 127.5) - 1  # Normalize to [-1,1] range\n",
    "\n",
    "    # Initialize latent vector z with random noise\n",
    "    z = torch.randn(1, 100, 1, 1, requires_grad=True)\n",
    "\n",
    "    # Define optimizer for z\n",
    "    optimizer = torch.optim.Adam([z], lr=lr)\n",
    "\n",
    "    for step in range(num_steps):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Generate image from z\n",
    "        generated_image = generator(z)\n",
    "\n",
    "        # Compute loss (Mean Squared Error between real and generated images)\n",
    "        loss = F.mse_loss(generated_image, image)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            print(f\"Step {step}/{num_steps} - Loss: {loss.item():.6f}\")\n",
    "\n",
    "    return z.detach()\n",
    "\n",
    "# Example usage:\n",
    "real_batch = next(iter(places_val_dataloader))  # Get a batch of images\n",
    "real_image = real_batch[0]  # Extract a single image\n",
    "real_image = real_image \n",
    "\n",
    "projected_z = project_image_to_latent(real_image, places_generator)\n",
    "print(\"Image projected onto latent space successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c13def-e39d-414b-a9f0-4b9b5382e0ce",
   "metadata": {},
   "source": [
    "# Perform Image Editing in the Latent Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f1926712-6980-4c20-954d-6845c933e702",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_latent_vector(z, generator, edit_direction, intensity=0.5):\n",
    "    \"\"\"\n",
    "    Modifies the latent vector in a specific direction to change color or shape.\n",
    "    \"\"\"\n",
    "    modified_z = z + intensity * edit_direction\n",
    "    modified_z = modified_z.detach().clone().requires_grad_(True)  # Ensure requires_grad is True\n",
    "\n",
    "    return modified_z\n",
    "\n",
    "# Example: Apply a small color change\n",
    "color_edit_direction = torch.randn_like(projected_z)  # Random color change direction\n",
    "modified_z = modify_latent_vector(projected_z, places_generator, color_edit_direction, intensity=0.2)\n",
    "\n",
    "# Generate the edited image\n",
    "edited_image = places_generator(modified_z)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b832b2f9-5328-4d2c-88dc-834035d13376",
   "metadata": {},
   "source": [
    "# Transfer Edits Back to High-Resolution Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e8e6d8b7-2948-488b-b401-cc46efec34ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def compute_optical_flow(img1, img2):\n",
    "    \"\"\"\n",
    "    Computes dense optical flow between two images using Farneback method.\n",
    "    \"\"\"\n",
    "    img1_gray = cv2.cvtColor(img1, cv2.COLOR_RGB2GRAY)\n",
    "    img2_gray = cv2.cvtColor(img2, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    flow = cv2.calcOpticalFlowFarneback(img1_gray, img2_gray, None, \n",
    "                                        0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "\n",
    "    return flow.astype(np.float32)  # Ensure float32 type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "29fbca90-9cd3-4371-beff-1b77cafe6ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edited high-resolution image saved successfully!\n"
     ]
    }
   ],
   "source": [
    "def apply_optical_flow(img, flow):\n",
    "    \"\"\"\n",
    "    Applies motion flow to warp the original image.\n",
    "    \"\"\"\n",
    "    h, w = flow.shape[:2]\n",
    "\n",
    "    # Create a grid of (x,y) coordinates\n",
    "    y_coords, x_coords = np.meshgrid(np.arange(h), np.arange(w), indexing='ij')\n",
    "\n",
    "    # Compute remap coordinates by adding flow vectors\n",
    "    map_x = (x_coords + flow[..., 0]).astype(np.float32)\n",
    "    map_y = (y_coords + flow[..., 1]).astype(np.float32)\n",
    "\n",
    "    # Warp image using flow\n",
    "    warped = cv2.remap(img, map_x, map_y, interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "    return warped\n",
    "\n",
    "# Convert tensors to numpy images\n",
    "real_img_np = (real_image.cpu().squeeze().permute(1, 2, 0).numpy() * 255).astype(np.uint8)\n",
    "edited_img_np = (edited_image.detach().cpu().squeeze().permute(1, 2, 0).numpy() * 255).astype(np.uint8)\n",
    "\n",
    "# Compute motion flow\n",
    "flow = compute_optical_flow(real_img_np, edited_img_np)\n",
    "\n",
    "# Apply flow to high-resolution original image\n",
    "final_high_res = apply_optical_flow(real_img_np, flow)\n",
    "\n",
    "cv2.imwrite(\"final_high_res_edit.jpg\", final_high_res)\n",
    "print(\"Edited high-resolution image saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4b8a8b-f57b-436f-885c-887c16dfee68",
   "metadata": {},
   "source": [
    "# Interactive UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "42cfa478-add2-4d4b-aca7-e8d0a5d1b2d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyqt5 in c:\\users\\shashwati\\anaconda3\\lib\\site-packages (5.15.10)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\shashwati\\anaconda3\\lib\\site-packages (4.10.0.84)\n",
      "Requirement already satisfied: numpy in c:\\users\\shashwati\\anaconda3\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: torch in c:\\users\\shashwati\\anaconda3\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\shashwati\\anaconda3\\lib\\site-packages (0.21.0)\n",
      "Requirement already satisfied: PyQt5-sip<13,>=12.13 in c:\\users\\shashwati\\anaconda3\\lib\\site-packages (from pyqt5) (12.13.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\shashwati\\anaconda3\\lib\\site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\shashwati\\anaconda3\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\shashwati\\anaconda3\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\shashwati\\anaconda3\\lib\\site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in c:\\users\\shashwati\\anaconda3\\lib\\site-packages (from torch) (2025.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\shashwati\\anaconda3\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\shashwati\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\shashwati\\anaconda3\\lib\\site-packages (from torchvision) (11.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\shashwati\\anaconda3\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~atplotlib (C:\\Users\\SHASHWATI\\anaconda3\\Lib\\site-packages)\n",
      "DEPRECATION: Loading egg at c:\\users\\shashwati\\anaconda3\\lib\\site-packages\\apache_beam-2.60.0-py3.11-win-amd64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\shashwati\\anaconda3\\lib\\site-packages\\avro_python3-1.10.2-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\shashwati\\anaconda3\\lib\\site-packages\\contextlib2-21.6.0-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\shashwati\\anaconda3\\lib\\site-packages\\lvis-0.5.3-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\shashwati\\anaconda3\\lib\\site-packages\\object_detection-0.1-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\shashwati\\anaconda3\\lib\\site-packages\\portalocker-2.10.1-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\shashwati\\anaconda3\\lib\\site-packages\\pycocotools-2.0.8-py3.11-win-amd64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\shashwati\\anaconda3\\lib\\site-packages\\pyparsing-2.4.7-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\shashwati\\anaconda3\\lib\\site-packages\\sacrebleu-2.2.0-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\shashwati\\anaconda3\\lib\\site-packages\\tensorflow_io-0.31.0-py3.11-win-amd64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\shashwati\\anaconda3\\lib\\site-packages\\tf_models_official-2.17.0-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "WARNING: Ignoring invalid distribution ~atplotlib (C:\\Users\\SHASHWATI\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~atplotlib (C:\\Users\\SHASHWATI\\anaconda3\\Lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install pyqt5 opencv-python numpy torch torchvision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2da1278e-1d6e-4d6d-bc14-e3f23c1df439",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PyQt5.QtWidgets import QApplication, QMainWindow, QLabel, QPushButton, QFileDialog, QVBoxLayout, QWidget\n",
    "from PyQt5.QtGui import QPixmap, QImage, QPainter, QPen\n",
    "from PyQt5.QtCore import Qt, QPoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7af92a40-fa84-4bce-8089-e199bbf3509f",
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SHASHWATI\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "class ImageEditor(QMainWindow):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.setWindowTitle(\"Realistic Image Manipulation UI\")\n",
    "        self.setGeometry(100, 100, 800, 600)\n",
    "\n",
    "        # UI Elements\n",
    "        self.image_label = QLabel(self)\n",
    "        self.image_label.setAlignment(Qt.AlignCenter)\n",
    "\n",
    "        self.load_button = QPushButton(\"Load Image\", self)\n",
    "        self.load_button.clicked.connect(self.load_image)\n",
    "\n",
    "        self.color_brush_button = QPushButton(\"Color Brush\", self)\n",
    "        self.color_brush_button.clicked.connect(self.use_color_brush)\n",
    "\n",
    "        self.warp_tool_button = QPushButton(\"Warp Tool\", self)\n",
    "        self.warp_tool_button.clicked.connect(self.use_warp_tool)\n",
    "\n",
    "        self.apply_gan_button = QPushButton(\"Apply GAN Edit\", self)\n",
    "        self.apply_gan_button.clicked.connect(self.apply_gan_edit)\n",
    "\n",
    "        self.save_button = QPushButton(\"Save Image\", self)\n",
    "        self.save_button.clicked.connect(self.save_image)\n",
    "\n",
    "        # Layout\n",
    "        layout = QVBoxLayout()\n",
    "        layout.addWidget(self.image_label)\n",
    "        layout.addWidget(self.load_button)\n",
    "        layout.addWidget(self.color_brush_button)\n",
    "        layout.addWidget(self.warp_tool_button)\n",
    "        layout.addWidget(self.apply_gan_button)\n",
    "        layout.addWidget(self.save_button)\n",
    "\n",
    "        container = QWidget()\n",
    "        container.setLayout(layout)\n",
    "        self.setCentralWidget(container)\n",
    "\n",
    "        # Image Variables\n",
    "        self.image = None\n",
    "        self.image_path = None\n",
    "        self.last_point = QPoint()\n",
    "        self.drawing = False\n",
    "        self.brush_color = (255, 0, 0)  # Default red brush\n",
    "        self.current_tool = None\n",
    "        self.places_generator = places_generator  # Ensure the generator is available in the class\n",
    "\n",
    "    def load_image(self):\n",
    "        \"\"\"Loads an image from disk and displays it.\"\"\"\n",
    "        options = QFileDialog.Options()\n",
    "        file_path, _ = QFileDialog.getOpenFileName(self, \"Load Image\", \"\", \"Images (*.png *.jpg *.jpeg)\", options=options)\n",
    "\n",
    "        if file_path:\n",
    "            self.image_path = file_path\n",
    "            self.image = cv2.imread(file_path)\n",
    "            self.image = cv2.cvtColor(self.image, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
    "            self.display_image()\n",
    "\n",
    "    def display_image(self):\n",
    "        \"\"\"Displays the current image on the UI.\"\"\"\n",
    "        if self.image is None:\n",
    "            return\n",
    "\n",
    "        h, w, ch = self.image.shape\n",
    "        bytes_per_line = ch * w\n",
    "        q_image = QImage(self.image.data, w, h, bytes_per_line, QImage.Format_RGB888)\n",
    "\n",
    "        pixmap = QPixmap.fromImage(q_image)\n",
    "        self.image_label.setPixmap(pixmap)\n",
    "\n",
    "    def use_color_brush(self):\n",
    "        \"\"\"Activates color brush tool.\"\"\"\n",
    "        self.current_tool = \"color_brush\"\n",
    "\n",
    "    def use_warp_tool(self):\n",
    "        \"\"\"Activates warp tool.\"\"\"\n",
    "        self.current_tool = \"warp_tool\"\n",
    "\n",
    "    def mousePressEvent(self, event):\n",
    "        \"\"\"Handles mouse press for drawing and warping.\"\"\"\n",
    "        if self.image is None or self.current_tool is None:\n",
    "            return\n",
    "\n",
    "        self.drawing = True\n",
    "        self.last_point = event.pos()\n",
    "\n",
    "    def mouseMoveEvent(self, event):\n",
    "        \"\"\"Handles drawing or warping on mouse movement.\"\"\"\n",
    "        if not self.drawing or self.image is None:\n",
    "            return\n",
    "\n",
    "        # Convert PyQt coordinates to NumPy image coordinates\n",
    "        x1, y1 = self.last_point.x(), self.last_point.y()\n",
    "        x2, y2 = event.pos().x(), event.pos().y()\n",
    "\n",
    "        h, w, _ = self.image.shape\n",
    "        x1, x2 = int(x1 * w / self.width()), int(x2 * w / self.width())\n",
    "        y1, y2 = int(y1 * h / self.height()), int(y2 * h / self.height())\n",
    "\n",
    "        if self.current_tool == \"color_brush\":\n",
    "            cv2.line(self.image, (x1, y1), (x2, y2), self.brush_color, thickness=5)\n",
    "\n",
    "        elif self.current_tool == \"warp_tool\":\n",
    "            dx = x2 - x1\n",
    "            dy = y2 - y1\n",
    "            self.image = self.warp_image(dx, dy)\n",
    "\n",
    "        self.last_point = event.pos()\n",
    "        self.display_image()\n",
    "\n",
    "    def mouseReleaseEvent(self, event):\n",
    "        \"\"\"Stops drawing or warping on mouse release.\"\"\"\n",
    "        self.drawing = False\n",
    "\n",
    "    def warp_image(self, dx, dy):\n",
    "        \"\"\"Applies a warp effect to the image based on user input.\"\"\"\n",
    "        h, w = self.image.shape[:2]\n",
    "        map_x, map_y = np.meshgrid(np.arange(w), np.arange(h), indexing='xy')\n",
    "\n",
    "        map_x = map_x.astype(np.float32) + dx\n",
    "        map_y = map_y.astype(np.float32) + dy\n",
    "\n",
    "        warped = cv2.remap(self.image, map_x, map_y, interpolation=cv2.INTER_LINEAR)\n",
    "        return warped\n",
    "\n",
    "    def apply_gan_edit(self):\n",
    "        \"\"\"Applies the trained DCGAN edits to the image.\"\"\"\n",
    "        if self.image is None:\n",
    "            return\n",
    "\n",
    "        # Resize image to 128x128 to match GAN output\n",
    "        resized_img = cv2.resize(self.image, (128, 128))\n",
    "\n",
    "        # Convert image to tensor\n",
    "        image_tensor = torch.tensor(resized_img).float().permute(2, 0, 1).unsqueeze(0)\n",
    "        image_tensor = (image_tensor / 127.5) - 1  # Normalize to [-1,1]\n",
    "\n",
    "        # Project to latent space\n",
    "        projected_z = torch.randn(1, 100, 1, 1, requires_grad=True)\n",
    "        optimizer = torch.optim.Adam([projected_z], lr=0.1)\n",
    "\n",
    "        for step in range(300):\n",
    "            optimizer.zero_grad()\n",
    "            generated_image = self.places_generator(projected_z)\n",
    "\n",
    "            # Ensure generated_image is 128x128 before calculating loss\n",
    "            loss = torch.nn.functional.mse_loss(generated_image, image_tensor)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Convert generated image to NumPy format\n",
    "        edited_image = generated_image.detach().cpu().squeeze().permute(1, 2, 0).numpy()\n",
    "        edited_image = ((edited_image + 1) * 127.5).astype(np.uint8)  # Convert back to [0,255]\n",
    "\n",
    "        # Resize back to original size\n",
    "        self.image = cv2.resize(edited_image, (self.image.shape[1], self.image.shape[0]))\n",
    "        self.display_image()\n",
    "\n",
    "    def save_image(self):\n",
    "        \"\"\"Saves the edited image.\"\"\"\n",
    "        if self.image is None:\n",
    "            return\n",
    "\n",
    "        save_path, _ = QFileDialog.getSaveFileName(self, \"Save Image\", \"\", \"Images (*.png *.jpg *.jpeg)\")\n",
    "        if save_path:\n",
    "            cv2.imwrite(save_path, cv2.cvtColor(self.image, cv2.COLOR_RGB2BGR))\n",
    "            print(\"Image saved successfully!\")\n",
    "\n",
    "# Run Application\n",
    "if __name__ == \"__main__\":\n",
    "    app = QApplication(sys.argv)\n",
    "    window = ImageEditor()\n",
    "    window.show()\n",
    "    sys.exit(app.exec_())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3d65cc-30a5-447d-8ffa-c65a83fe14b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ebb723-374d-4f41-bb1b-4eec5dc7b388",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
